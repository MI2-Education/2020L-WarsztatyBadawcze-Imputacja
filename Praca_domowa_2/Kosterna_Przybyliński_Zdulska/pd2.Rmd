---
title: "The imputation methods big test"
author: "Jakub Kosterna, Dawid Przybyli≈Ñski & Hanna Zdulska"
date: "22/04/2020"
output:
  html_document:
    code_folding: hide
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

## Abstract

Choosing the best-suited imputation is the daily dilemma of every data scientist. Some of them believe the crux lies in the most advanced and sophisticated, the others trust the simplest of all possible.

In this document we will try to find the one, objectively finest imputation method by testing five of popular ones on forteen specially selected data sets prepared for these operations.

## Imputation functions

We'll look at the two simple imputation methods with the following short markings:

1. *modeMedian* - missing data supplemented with medians from individual columns
2. *removeRows* - all the rows with any nonexistent values removed

```{r imputation_basic_functions, cache = TRUE}
imputation_mode_median <- function(df){
  
  Mode <- function(x) {
    ux <- unique(x)
    ux[which.max(tabulate(match(x, ux)))]
  }
  
  for (i in 1L:length(df)){
    if (sum(is.na(df[,i])) > 0){
      if (mode(df[,i]) == 'character' | is.factor(df[,i])){
        to_imp <- Mode(df[,i])
        df[,i][is.na(df[,i])] <- to_imp
      }
      else{
        to_imp <- median(df[,i], na.rm = TRUE) 
        df[,i][is.na(df[,i])] <- to_imp
      }
    }
  }
  
  return(df)
}
imputation_remove_rows <- function(df){
  return (na.omit(df))
}
```

... and also three more advanced algorithms from popular libraries:

3. *mice* ^1^ - standard imputation with *maxit* parameter of value 0 and default 5 number of imputed datasets
4. *VIM* ^2^ - default *kNN* implementation
5. *missForest* ^3^ - simple imputation offered by the package

```{r imputation_advanced_functions, message = FALSE, warning = FALSE, cache = TRUE}
library(mice)
imputation_fun_mice <- function(df){
  init <- mice(df, maxit=0) 
  meth <- init$method
  predM <- init$predictorMatrix
  imputed <- mice(df, method=meth, predictorMatrix=predM, m=5)
  completed <- complete(imputed)
  return(completed)
}
library(VIM)
imputation_fun_vim <- function(df){
  no_columns <- length(df)
  imputed <- kNN(df)
  imputed <- imputed[,1:no_columns]
  return(imputed)
}
library(missForest)
imputation_fun_missForest <- function(df){
  return(missForest(df)$ximp)
}

```

## Reading datasets

The fourteen data frames on which we will test these above were taken from **OpenML100 collection** and were corrected specifically for this research. They can be found under the following identifiers with the following names:

* 1590 - *adult*
* 188 - *eucalyptus*
* 23381 - *dresses-sales*
* 29 - *credit-approval*
* 38 - *sick*
* 40536 - *SpeedDating*
* 41278 - *okcupid-stem*
* 56 - *vote*
* 6332 - *cylinder-bands*
* 1018 - *ipums_la_99-small*
* 27 - *colic*
* 4 - *labor*
* 55 - *hepatitis*
* 944 - *echoMonths*

Those above have been placed in individual directories identified by id in the prepared directory.

```{r read_dataset, cache = TRUE}
DFT_REPO_DATASET_DIR = './dependencies/datasets'
read_dataset <- function(openml_id, dataset_dir = DFT_REPO_DATASET_DIR){
  
  if (!dir.exists(dataset_dir)){
    stop(paste(dataset_dir, 'does not exist' ))
  }
  
  dir <- paste(dataset_dir, paste('openml_dataset', openml_id, sep = '_'), sep ='/')
  if (!dir.exists(dir)){
    stop(paste(dir, 'does not exist' ))
  }
  
  start_dir <- getwd()
  
  # set right dir to code.R to acually work - it depends on dirlocation to create json
  setwd(dir)
  # use new env to avoid trashing globalenv
  surogate_env <- new.env(parent = .BaseNamespaceEnv)
  attach(surogate_env)
  source("code.R",surogate_env)
  
  j <- jsonlite::read_json('./dataset.json')
  j$dataset <- surogate_env$dataset
  setwd(start_dir)
  
  return(j)
}
```

In order to read all datasets we will use another function, which will return a dataframe containing all the important informations about the test matrices.

```{r read_all_datasets, cache = TRUE}
read_all_datasets <- function(dataset_dir = DFT_REPO_DATASET_DIR){
  
  if (!dir.exists(dataset_dir)){
    stop(paste(dataset_dir, 'does not exist'))
  }
  
  start_dir <- getwd()
  subdirs <- dir(dataset_dir)
  ids <- sapply(subdirs, function(dir){substr(dir, 16, nchar(dir))})
  datasets_combined <- lapply(ids, function(x){read_dataset(x, dataset_dir)})
  
  datasets_combined <- t(datasets_combined)
  return(unname(t(datasets_combined)))
}
```

All of datasets contain missing information denoted as NA and differ in size, number of features, number of observations and percentage of missing data. Table below decribes those informations.

```{r dataset description, message = FALSE, warning=FALSE, cache = TRUE}

dfs <- read_all_datasets()

ids <- lapply(dfs, function(x){x$id})
names <- lapply(dfs, function(x){x$data_name})
n_inst <- lapply(dfs, function(x){x$number_of_instances})
n_features <- lapply(dfs, function(x){x$number_of_features})
number_of_numeric_features <- lapply(dfs, function(x){x$number_of_numeric_features})
number_of_categorical_features <- lapply(dfs, function(x){x$number_of_categorical_features})
number_of_missings <- lapply(dfs, function(x){x$number_of_missings})
proc_missing <- lapply(dfs, function(x){x$number_of_missing/(x$number_of_instances * x$number_of_features)})
number_of_instances_with_missings <- lapply(dfs, function(x){x$number_of_instances_with_missings})
number_of_features_with_missings <- lapply(dfs, function(x){x$number_of_features_with_missings})

#fix wrong json
names[[10]] <- "okcupid-stem"

table <- data.frame("id" = unlist(ids), 
                    "name" = unlist(names),
                    "number_of_instances" = unlist(n_inst),
                    "number_of_features" = unlist(n_features),
                    "number_of_missing" = unlist(number_of_missings),
                    "procentage_of_missing" = round(unlist(proc_missing)*100,2))

knitr::kable(table)
```


## Metrics functions

In order to test the same test-train splits we'll use one random seed 1357 for all datasets.

```{r train_test_split, cache = TRUE}
set.seed(1357)
train_test_split <- function(dataset, train_size){
  smp_size <- floor(train_size * nrow(dataset))
  typeof(smp_size)
  
  train_ind <- sample(seq_len(nrow(dataset)), size = smp_size)
  
  train <- dataset[train_ind, ]
  test <- dataset[-train_ind, ]
  
  return (list(train, test))
}
```

For each machine learning model after every imputation we will get the confusion matrix and the values of four basic metrics:

* *accuracy* - $\frac{TP+TN}{TP+FP+FN+TN}$
* *precision* - $\frac{TP}{TP+FP}$
* *recall* - $\frac{TP}{TP+FN}$
* *f1* - $2*\frac{Recall * Precision}{Recall + Precision}$

These four will give us good information about the quality of imputation and give us a good comparison. We get simple and clear information about which observations are well classified, which ones are wrong and how they should be. Considering the number of sets to consider and their different properties, even when it comes to balancing, comparing these four measures, the fact that it will show the pros and cons of given imputations, will additionally be good to compare them.

```{r metrics, cache = TRUE}
get_confusion_matrix <- function(test, pred){
  return (table(Truth = test, Prediction = pred))
}
confusion_matrix_values <- function(confusion_matrix){
  TP <- confusion_matrix[2,2]
  TN <- confusion_matrix[1,1]
  FP <- confusion_matrix[1,2]
  FN <- confusion_matrix[2,1]
  return (c(TP, TN, FP, FN))
}
accuracy <- function(confusion_matrix){
  conf_matrix <- confusion_matrix_values(confusion_matrix)
  return((conf_matrix[1] + conf_matrix[2]) / (conf_matrix[1] + conf_matrix[2] + conf_matrix[3] + conf_matrix[4]))
}
precision <- function(confusion_matrix){
  conf_matrix <- confusion_matrix_values(confusion_matrix)
  return(conf_matrix[1]/ (conf_matrix[1] + conf_matrix[3]))
}
recall <- function(confusion_matrix){
  conf_matrix <- confusion_matrix_values(confusion_matrix)
  return(conf_matrix[1] / (conf_matrix[1] + conf_matrix[4]))
}
f1 <- function(confusion_matrix){
  conf_matrix <- confusion_matrix_values(confusion_matrix)
  rec <- recall(confusion_matrix)
  prec <- precision(confusion_matrix)
  return(2 * (rec * prec) / (rec + prec))
}
```

## Imputation results

The results of our work will be checked using the implemented classification tree algorithm from the *rpart ^4^* package. We have chosen this algorithm because this classifier works well for almost any type of data - and for the most part numerical, and for the most part factors, it can handle and make a decent model. The use of an algorithm using, for example, the distance from the nearest points or separating sets by clusters would not be recommended for each data frame, and given the diversity, here rpart should do a good job.

```{r get_result_function, warning = FALSE, message = FALSE, cache = TRUE}
library(rpart)
get_result <- function(dataset_list, imputation_fun){
  
dataset <- dataset_list$dataset
name_of_target <- dataset_list$target
# train test split
train_test <- train_test_split(imputated_dataset, 0.8)
train <- as.data.table(train_test[1])
test <- as.data.table(train_test[2])
# imputation
imputation_start = Sys.time() # start to measure time
train <- imputation_fun(train)
test <- imputation_fun(test)
imputation_stop = Sys.time() # end measuring time
imputed_dataset <- cbind(train, test)
# modelling
vars <- colnames(dataset)[colnames(dataset)!=name_of_target]
my_formula <- as.formula(paste(name_of_target, paste(vars, collapse=" + "), sep=" ~ "))
modelling_start = Sys.time() # start to measure time
tree_model <- rpart(formula = my_formula, data = train,
                    method = "class", control = rpart.control(cp = 0))
y_pred <- as.data.frame(predict(tree_model, test, type = "class"))
modelling_stop = Sys.time() # end measuring time
# calculating metrics
confusion_matrix <- get_confusion_matrix(test[[name_of_target]], y_pred[,1])
accuracy_v <- accuracy(confusion_matrix)
precision_v <- precision(confusion_matrix)
recall_v <- recall(confusion_matrix)
f1_v <- f1(confusion_matrix)
classification_report <- data.frame(accuracy_v, precision_v,
                                    recall_v, f1_v)
colnames(classification_report) <- c("accuracy", "precision",
                                     "recall", "f1")
dataset_list$dataset <- NULL
# in future maybe return all dataset_list ?
# for now stick with readability
imp_method_name <- deparse(substitute(imputation_fun))
return(list( dataset_id = dataset_list$id, 
             imp_method = imp_method_name,
             confusion_matrix = confusion_matrix,
             classification_report = classification_report,
             imputation_time = imputation_stop - imputation_start,
             modelling_time = modelling_stop - modelling_start))
}
data_all <- read_all_datasets()
# imputations and targets preparation
imputations <- list(imputation_fun_vim, imputation_fun_missForest,
                  imputation_remove_rows, imputation_mode_median, imputation_fun_mice)
targets <- lapply(data_all, function(d){d$target})
```

Also, it is worth mentioning that removing rows computes a different dataset compared to other techniques. Because of that one needs to be cautious while analysing and comparing remove_rows to other imputation methods. For one's own research it is probably worth replacing removing rows with removing columns for datasets with few features containing missing values. 
Here are the results. NA indicates that method returned error:

```{r warning=FALSE, cache = TRUE}
  library(knitr)
  results <- rep(list(0), 5)
  results[[1]] <- readRDS('./part_results/res1_new.rds')
  results[[2]] <- readRDS('./part_results/res2_new.rds')
  results[[3]] <- readRDS('./part_results/res3_new.rds')
  results[[4]] <- readRDS('./part_results/res4_new.rds')
  results[[5]] <- readRDS('./part_results/res5_new.rds')
  
  result_table <- function(ds_it){
    d <- data.frame(matrix(ncol = 7, nrow = 0))
    colnames(d) <- c("imputation", "accuracy", "precision", "recall", "f1", "imp_time", "mod_time")
    for (i in 1:length(imputations)){
      if (length(results[[i]][[ds_it]]) != 1){ # length equal 1 means "ERROR" was generated in partial results 
        d[i,] <- c(results[[i]][[ds_it]]$imp_method,
               round(as.numeric(results[[i]][[ds_it]]$classification_report$accuracy),3),   
               round(as.numeric(results[[i]][[ds_it]]$classification_report$precision),3),
               round(as.numeric(results[[i]][[ds_it]]$classification_report$recall),3),
               round(as.numeric(results[[i]][[ds_it]]$classification_report$f1),3),
               round(as.numeric(results[[i]][[ds_it]]$imputation_time),3),
               # @TODO konwersja jednostek, as.numeric nie rozroznia jednostek czasu
               round(as.numeric(results[[i]][[ds_it]]$modelling_time),3)) 
      }
      else{ # for error input imputation name
        if (i==3){
          d[i,1] <- "imputation_fun_mice" 
        }
        if (i==4){
          d[i,1] <- "imputation_fun_missForest" 
        }
        if (i==5){
          d[i,1] <- "imputation_fun_vim" 
        }
      }
    }
    d
  }
  
  # change i bound to number of datasets 
  #for (i in 1:length(results[[1]])){ # won't use it, because the output looks bad
  #  kable(result_table(i), caption = paste("Dataset id: ", results[[2]][[i]]$dataset_id, sep = ""))
  #}
  
  kable(result_table(1), caption = paste("Dataset id: ", results[[2]][[1]]$dataset_id, sep = ""))
  kable(result_table(2), caption = paste("Dataset id: ", results[[2]][[2]]$dataset_id, sep = ""))
  kable(result_table(3), caption = paste("Dataset id: ", results[[2]][[3]]$dataset_id, sep = ""))
  kable(result_table(4), caption = paste("Dataset id: ", results[[2]][[4]]$dataset_id, sep = ""))
  kable(result_table(5), caption = paste("Dataset id: ", results[[2]][[5]]$dataset_id, sep = ""))
  kable(result_table(6), caption = paste("Dataset id: ", results[[2]][[6]]$dataset_id, sep = ""))
  kable(result_table(7), caption = paste("Dataset id: ", results[[2]][[7]]$dataset_id, sep = ""))
  kable(result_table(8), caption = paste("Dataset id: ", results[[2]][[8]]$dataset_id, sep = ""))
  kable(result_table(9), caption = paste("Dataset id: ", results[[2]][[9]]$dataset_id, sep = ""))
  kable(result_table(10), caption = paste("Dataset id: ", results[[2]][[10]]$dataset_id, sep = ""))
  kable(result_table(11), caption = paste("Dataset id: ", results[[2]][[11]]$dataset_id, sep = ""))
  kable(result_table(12), caption = paste("Dataset id: ", results[[2]][[12]]$dataset_id, sep = ""))
  kable(result_table(13), caption = paste("Dataset id: ", results[[2]][[13]]$dataset_id, sep = ""))
  kable(result_table(14), caption = paste("Dataset id: ", results[[2]][[14]]$dataset_id, sep = ""))
```

## Comparing imputation times

Let's build a dataframe containing information about imputation methods.

```{r imputation_times_counting, cache = TRUE}
ndatasets <- length(results[[1]])
nimputations <- length(imputations)
imputation_names <- c("remove", "median", "mice", "missForest", "VIM")
dataset_ids <- rep(NULL, ndatasets)
for(i in 1:ndatasets){
  dataset_ids[i] <- results[[2]][[i]]$dataset_id
}

imputation_times <- data.frame(rep(0, ndatasets))
for (imputation_id in 1:nimputations){
  imp_id_times <- rep(NULL, ndatasets)
  
  for(dataset_id in 1:ndatasets){
    if("imputation_time" %in% names(results[[imputation_id]][[dataset_id]])){
      imp_time <- results[[imputation_id]][[dataset_id]]$imputation_time
      imp_time <- as.numeric(imp_time, units="secs")
      imp_id_times[dataset_id] <- imp_time
    }
  }
  imputation_times <- cbind(imputation_times, imp_id_times)
}

imputation_times[1] <- dataset_ids
colnames(imputation_times) <- c("dataset_id", imputation_names)
imputation_times$dataset_id <- as.factor(imputation_times$dataset_id)
knitr::kable(imputation_times)
```

Looks good! Now it's visualization time.

```{r imputation_times_plot, message = FALSE, warning = FALSE, cache = TRUE}
library(ggplot2)
# install.packages("reshape") # if not installed
library(reshape)

# in order to connect colors and shapes in legend see:
# https://stackoverflow.com/questions/12410908/combine-legends-for-color-and-shape-into-a-single-legend

colors <- c("remove" = "orange", "median" = "cyan", "mice" = "green",
            "missForest" = "red", "VIM" = "blue")
imptimes <- melt(imputation_times)

require(scales)
ggplot(data=imptimes, aes(x = dataset_id, y = value, color = variable, shape = variable)) +
  geom_point() + 
  ggtitle("Datasets' imputations times") +
  theme_gray() +
  scale_y_continuous(trans = log2_trans(),
    breaks = trans_breaks("log2", function(x) 2^x),
    labels = trans_format("log2", math_format(2^.x))) +
  labs(x = "Dataset Id",
       y = "Imputation time [in seconds]",
       color = "Legend") +
  scale_color_manual(name = "imputation name", values = colors) +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5)) +
  #scale_colour_manual(name = "Treatment & State",
  #                    labels = c("Control, Non-F", "Control, Flwr", "Exclosure, Non-F", "Exclosure, Flwr"),
  #                    values = c("blue", "red", "blue", "red")) +   
  scale_shape_manual(name = "imputation name",
                     values = 15:19L)



```

Judging by the logarithmic scale, no surprise removing rows and median replenishment are definitely the fastest methods, with removing rows being several times faster. Looking at more advanced ones, definitely *VIM* rules - probably usually something like 5-10 times faster than *mice* and *missForest*. These last two are quite slow, with the missforest appearing to be slightly faster.

What do the numbers say?

```{r imputation_times_measures, cache = TRUE}
means <- rep(NA, nimputations)
medians <- rep(NA, nimputations)
stDeviations <- rep(NA, nimputations)

for(i in (1:nimputations)){
  means[i] <- mean(imputation_times[imputation_names[i]][!is.na(imputation_times[imputation_names[i]])])
  means[i] <- round(means[i], 3)
  medians[i] <- median(imputation_times[imputation_names[i]][!is.na(imputation_times[imputation_names[i]])])
  medians[i] <- round(medians[i], 3)
  stDeviations[i] <- sd(imputation_times[imputation_names[i]][!is.na(imputation_times[imputation_names[i]])])
  stDeviations[i] <- round(stDeviations[i], 3)
}

imputation_times_measures <- data.frame(imputation_names, means, medians, stDeviations)
knitr::kable(imputation_times_measures)
```

Taking into account all the imputations which have been implemented, we can clearly see that two easy ones really stand out. However, we cannot fully compare the other three - due to the fact that missForest failed on three data sets, and mice - up to seven.

Considering such a large spread of data size, it is very interesting difference between the median and the average for VIM - the first is almost seven minutes, the second - barely one and a half seconds. In general, however, it is certainly much longer than methods for removing incomplete rows and filling with median.

Let's count these four measures again, but only for those datasets for which all imputations were successful - these are ids 27, 38, 55, 56, 944, 188  (dataset with id 4 softened on removing rows containing any missing items, because each of its poems had some missing items).

```{r imputation_times_measures_little, cache = TRUE}
dataset_ids_fullimp <- c(2, 5, 8, 11, 13, 14)
ndatasets_fullimp <- length(dataset_ids_fullimp)

means <- rep(NA, nimputations)
medians <- rep(NA, nimputations)
stDeviations <- rep(NA, nimputations)

for(i in 1:nimputations){
  means[i] <- mean(imputation_times[imputation_names[i]][dataset_ids_fullimp,])
  means[i] <- round(means[i], 3)
  medians[i] <- median(imputation_times[imputation_names[i]][dataset_ids_fullimp,])
  medians[i] <- round(medians[i], 3)
  stDeviations[i] <- sd(imputation_times[imputation_names[i]][dataset_ids_fullimp,])
  stDeviations[i] <- round(stDeviations[i], 3)
}

imputation_times_measures_fullimp <- data.frame(imputation_names, means, medians, stDeviations)
knitr::kable(imputation_times_measures_fullimp)
```

Now we have better results to compare. Taking into account the mean time, definitely *missForest* is the slowest, but also its standard deviation is incomparably huge - this is probably due to the fact that for smaller sets it is doing well, but due to its complexity, its slowdown can be seen for very large datasets. As shown in the chart, *VIM* is definitely better for quick calculations than *missForest* and *mice*, and considering the median, *mice* is comparable to *missForest* - so you can expect that for small data sets there is not much difference between them, and a lot of time we definitely need to devote to these larger data frames.

## Comparing modeling times

Let's also take a look at modeling times.

```{r modeling_times_counting, cache = TRUE}
modeling_times <- data.frame(rep(0, ndatasets))

for (imputation_id in 1:nimputations){
  mod_id_times <- rep(NULL, ndatasets)
  
  for(dataset_id in 1:ndatasets){
    if("imputation_time" %in% names(results[[imputation_id]][[dataset_id]])){
      mod_time <- results[[imputation_id]][[dataset_id]]$modelling_time
      mod_time <- as.numeric(mod_time, units="secs")
      mod_id_times[dataset_id] <- mod_time
    }
  }
  modeling_times <- cbind(modeling_times, mod_id_times)
}

modeling_times[1] <- dataset_ids
colnames(modeling_times) <- c("dataset_id", imputation_names)
imputation_times$dataset_id <- as.factor(modeling_times$dataset_id)
knitr::kable(modeling_times)
```

Very interesting! The longest modeling time took one of the largest sets... on deficiencies supplemented by median. And it took apparently longer than other modeling! could a random tree have more to do with a lot of the same values? Probably yes. Let's visualize our results.

```{r modeling_times_plot, message = FALSE, warning = FALSE, cache = TRUE}
modeling_times$dataset_id <- as.factor(modeling_times$dataset_id)
modtimes <- melt(modeling_times)

require(scales)
ggplot(data=modtimes, aes(x = dataset_id, y = value, color = variable, shape = variable)) +
  geom_point() + 
  ggtitle("Datasets' modeling times") +
  theme_light() +
  scale_y_continuous(trans = log2_trans(),
    breaks = trans_breaks("log2", function(x) 2^x),
    labels = trans_format("log2", math_format(2^.x))) +
  labs(x = "Dataset Id",
       y = "Modeling time [in seconds]",
       color = "Legend") +
  scale_color_manual(name = "imputation name", values = colors) +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5))
```

Here the results seem to be way more scattered. Let's check on measures also.

```{r modeling_times_measures, cache = TRUE}
means <- rep(NA, nimputations)
medians <- rep(NA, nimputations)
stDeviations <- rep(NA, nimputations)

for(i in (1:nimputations)){
  means[i] <- mean(modeling_times[imputation_names[i]][!is.na(modeling_times[imputation_names[i]])])
  means[i] <- round(means[i], 3)
  medians[i] <- median(modeling_times[imputation_names[i]][!is.na(modeling_times[imputation_names[i]])])
  medians[i] <- round(medians[i], 3)
  stDeviations[i] <- sd(modeling_times[imputation_names[i]][!is.na(modeling_times[imputation_names[i]])])
  stDeviations[i] <- round(stDeviations[i], 3)
}

modeling_times_measures <- data.frame(imputation_names, means, medians, stDeviations)
knitr::kable(modeling_times_measures)
```

The powerful but also fast tree classifier from *rpart* (both looking at the median and average) counted everything in less than a second. The largest fluctuations can be observed for the method of *median* and *VIM* supplementation - for them the standard deviation is less than two and a half seconds. You can also see very similar all three average measures for these two, probably due to the operation of *VIM* as a result giving similar effects as the *median*. Interestingly, unbeatably the fastest tree was built for the *mice* algorithm ... but wait! Maybe we'd better compare only the imputations for those sets for which we've done everything.

```{r modeling_times_measures_little, cache = TRUE}
means <- rep(NA, nimputations)
medians <- rep(NA, nimputations)
stDeviations <- rep(NA, nimputations)

for(i in 1:nimputations){
  means[i] <- mean(modeling_times[imputation_names[i]][dataset_ids_fullimp,])
  means[i] <- round(means[i], 3)
  medians[i] <- median(modeling_times[imputation_names[i]][dataset_ids_fullimp,])
  medians[i] <- round(medians[i], 3)
  stDeviations[i] <- sd(modeling_times[imputation_names[i]][dataset_ids_fullimp,])
  stDeviations[i] <- round(stDeviations[i], 3)
}

modeling_times_measures_fullimp <- data.frame(imputation_names, means, medians, stDeviations)
knitr::kable(modeling_times_measures_fullimp)
```

On the attached non-manipulated table, all average measures are almost identical. The conclusion is that the method of imputation probably has almost no effect on the modeling time - although it is difficult to say, given that we had a fairly small number of sets to compare and we have values in the order of hundredths of a second - probably just the operation of the computer pretty badly distorts the result.

## Best measures

Let us draw attention to our measures. How did our four measures work on them?

Attention! We will not consider sets with id 41278 and 188 in the next steps - they are non-binary classifications, so they are not affected by accuracy, precision, recall and f1. However, from the remaining twelve collections - we will be able to draw many valuable conclusions.

```{r measures_tables, cache = TRUE}
bin_ids <- c(1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14)

accuracies <- data.frame(rep(0, ndatasets))
precisions <- data.frame(rep(0, ndatasets))
recalls <- data.frame(rep(0, ndatasets))
f1s <- data.frame(rep(0, ndatasets))

for (imputation_id in 1:nimputations){
  acc_id <- rep(NA, ndatasets)
  pre_id <- rep(NA, ndatasets)
  rec_id <- rep(NA, ndatasets)
  f1_id <- rep(NA, ndatasets)
  
  for(dataset_id in bin_ids){
    if("classification_report" %in% names(results[[imputation_id]][[dataset_id]])){
      accuracy <- results[[imputation_id]][[dataset_id]]$classification_report$accuracy
      acc_id[dataset_id] <- accuracy
      precision <- results[[imputation_id]][[dataset_id]]$classification_report$precision
      pre_id[dataset_id] <- precision
      recall <- results[[imputation_id]][[dataset_id]]$classification_report$recall
      rec_id[dataset_id] <- recall
      f1 <- results[[imputation_id]][[dataset_id]]$classification_report$f1
      f1_id[dataset_id] <- f1
    }
  }
  accuracies <- cbind(accuracies, acc_id)
  precisions <- cbind(precisions, pre_id)
  recalls <- cbind(recalls, rec_id)
  f1s <- cbind(f1s, f1_id)
}

accuracies[1] <- dataset_ids
colnames(accuracies) <- c("dataset_id", imputation_names)
accuracies$dataset_id <- as.factor(accuracies$dataset_id)
accuracies <- accuracies[-c(2, 7), ] # delete  non-binary classification datasets
knitr::kable(accuracies, caption = "Accuracies")

precisions[1] <- dataset_ids
colnames(precisions) <- c("dataset_id", imputation_names)
precisions$dataset_id <- as.factor(precisions$dataset_id)
precisions <- precisions[-c(2, 7), ] # delete  non-binary classification datasets
knitr::kable(precisions, caption = "Precisions")

recalls[1] <- dataset_ids
colnames(recalls) <- c("dataset_id", imputation_names)
recalls$dataset_id <- as.factor(recalls$dataset_id)
recalls <- recalls[-c(2, 7), ] # delete  non-binary classification datasets
knitr::kable(recalls, caption = "Recalls")

f1s[1] <- dataset_ids
colnames(f1s) <- c("dataset_id", imputation_names)
f1s$dataset_id <- as.factor(f1s$dataset_id)
f1s <- f1s[-c(2, 7), ] # delete  non-binary classification datasets
knitr::kable(f1s, caption = "F1s")
```

Much data!!

Naturally - we'll visualize it.

```{r measures_visualizations, message = FALSE, warning = FALSE, cache = TRUE}
accMelt <- melt(accuracies)
accPlot <- ggplot(data=accMelt, aes(x = dataset_id, y = value, color = variable)) +
  geom_point() + 
  ggtitle("Datasets' accuracies") +
  ylim(0, 1) +
  labs(x = "Dataset Id",
       y = "Accuracy",
       color = "Legend") +
  scale_color_manual(name = "accuracy", values = colors) +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5))
accPlot

preMelt <- melt(precisions)
prePlot <- ggplot(data=preMelt, aes(x = dataset_id, y = value, color = variable)) +
  geom_point() + 
  ggtitle("Datasets' precisions") +
  ylim(0, 1) +
  labs(x = "Dataset Id",
       y = "Precision",
       color = "Legend") +
  scale_color_manual(name = "precision", values = colors) +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5))
prePlot

recMelt <- melt(recalls)
recPlot <- ggplot(data=recMelt, aes(x = dataset_id, y = value, color = variable)) +
  geom_point() + 
  ggtitle("Datasets' recalls") +
  ylim(0, 1) +
  labs(x = "Dataset Id",
       y = "Recall",
       color = "Legend") +
  scale_color_manual(name = "recall", values = colors) +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5))
recPlot

f1Melt <- melt(f1s)
f1Plot <- ggplot(data=f1Melt, aes(x = dataset_id, y = value, color = variable)) +
  geom_point() + 
  ggtitle("Datasets' f1s") +
  ylim(0, 1) +
  labs(x = "Dataset Id",
       y = "F1",
       color = "Legend") +
  scale_color_manual(name = "f1", values = colors) +
  theme(legend.position="right",
        plot.title = element_text(hjust = 0.5))
f1Plot

library(gridExtra)
grid.arrange(accPlot +
               ggtitle("Accuracies") +
               theme(legend.position = "none",
                axis.line.x = element_blank(),
                axis.ticks.x = element_blank(),
                axis.text.x = element_blank(),
                axis.title = element_blank()),
              prePlot +
               ggtitle("Precisions") +
               theme(legend.position = "none",
                axis.line.x = element_blank(),
                axis.ticks.x = element_blank(),
                axis.text.x = element_blank(),
                axis.title = element_blank()),
              recPlot +
               ggtitle("Recalls") +
               theme(legend.position = "none",
                axis.line.x = element_blank(),
                axis.ticks.x = element_blank(),
                axis.text.x = element_blank(),
                axis.title = element_blank()),
             f1Plot +
               ggtitle("F1s") +
               theme(legend.position = "none",
                axis.line.x = element_blank(),
                axis.ticks.x = element_blank(),
                axis.text.x = element_blank(),
                axis.title = element_blank()),
             nrow = 2,
             top = "Measures values for different datasets' imputations")
```

... where colors of points represent: <p style="color:orange">*1. **remove***</p>
<p style="color:cyan">*2. **median***</p><p style="color:green">*3. **mice***</p>
<p style="color:red">*4. **missForest***</p><p style="color:blue">*5. **VIM***</p>

Looking only at these plots, it is difficult to come up with something at first glance - for each time I come to some conclusion, results for a few other datasets that I did not pay attention to deny it. For sure we can already say that the scatter of results is rather large and analyzing the measures of concentration, we probably will not clearly indicate the best or the worst imputation.

Visualizations often help and ever give answers to the questions, but this time the best option will be definitely to calculate some measures. What means, medians and standard deviations give these measures?

```{r measures_measures, cache = TRUE}
accMeans <- rep(NA, nimputations)
accMedians <- rep(NA, nimputations)
accStDeviations <- rep(NA, nimputations)

preMeans <- rep(NA, nimputations)
preMedians <- rep(NA, nimputations)
preStDeviations <- rep(NA, nimputations)

recMeans <- rep(NA, nimputations)
recMedians <- rep(NA, nimputations)
recStDeviations <- rep(NA, nimputations)

f1sMeans <- rep(NA, nimputations)
f1sMedians <- rep(NA, nimputations)
f1sStDeviations <- rep(NA, nimputations)

for(i in (1:nimputations)){
    accMeans[i] <- mean(accuracies[imputation_names[i]][!is.na(accuracies[imputation_names[i]])])
    accMeans[i] <- round(accMeans[i], 3)
    accMedians[i] <- median(accuracies[imputation_names[i]][!is.na(accuracies[imputation_names[i]])])
    accMedians[i] <- round(accMedians[i], 3)
    accStDeviations[i] <- sd(accuracies[imputation_names[i]][!is.na(accuracies[imputation_names[i]])])
    accStDeviations[i] <- round(stDeviations[i], 3)
    
    preMeans[i] <- mean(precisions[imputation_names[i]][!is.na(precisions[imputation_names[i]])])
    preMeans[i] <- round(preMeans[i], 3)
    preMedians[i] <- median(precisions[imputation_names[i]][!is.na(precisions[imputation_names[i]])])
    preMedians[i] <- round(preMedians[i], 3)
    preStDeviations[i] <- sd(precisions[imputation_names[i]][!is.na(precisions[imputation_names[i]])])
    preStDeviations[i] <- round(preStDeviations[i], 3)
    
    recMeans[i] <- mean(recalls[imputation_names[i]][!is.na(recalls[imputation_names[i]])])
    recMeans[i] <- round(recMeans[i], 3)
    recMedians[i] <- median(recalls[imputation_names[i]][!is.na(recalls[imputation_names[i]])])
    recMedians[i] <- round(recMedians[i], 3)
    recStDeviations[i] <- sd(recalls[imputation_names[i]][!is.na(recalls[imputation_names[i]])])
    recStDeviations[i] <- round(recStDeviations[i], 3)
    
    f1sMeans[i] <- mean(f1s[imputation_names[i]][!is.na(f1s[imputation_names[i]])])
    f1sMeans[i] <- round(f1sMeans[i], 3)
    f1sMedians[i] <- median(f1s[imputation_names[i]][!is.na(f1s[imputation_names[i]])])
    f1sMedians[i] <- round(f1sMedians[i], 3)
    f1sStDeviations[i] <- sd(f1s[imputation_names[i]][!is.na(f1s[imputation_names[i]])])
    f1sStDeviations[i] <- round(f1sStDeviations[i], 3)
}

measures_measures <- data.frame(imputation_names, accMeans, accMedians, accStDeviations,
                                preMeans, preMedians, preStDeviations,
                                recMeans, recMedians, recStDeviations,
                                f1sMeans, f1sMedians, f1sStDeviations)

knitr::kable(measures_measures, content = "Model measures' concentrations' measures")
```

Looking at the table quickly, we can probably set our podium!! *MissForest* becomes the big winner just before ex aequo in second place: *mice* and *VIM*. At the bottom we have *median*, slightly overtaken by *remove_rows*? Well no! Remember that we should not take datasets for such comparison, for which we have only partial results. They probably strongly disturb the information about the data. The table above is not useless, because in the end we have means, medians and standard deviations of everything we could; but in order to be able to compare them without remorse, we should take only these five - which have complete information - we will calculate again the measures for datasets with ids 38, 56, 27, 55 and 944. This is necessary, because if not for the sets in which, naturally, regardless of the model or imputation we get very high or very low results generate a lot of chaos.

```{r correct_measures_measures, cache = TRUE}
ids_all_model_measures <- c(4, 6, 9, 11, 12)

accMeans <- rep(NA, nimputations)
accMedians <- rep(NA, nimputations)
accStDeviations <- rep(NA, nimputations)

preMeans <- rep(NA, nimputations)
preMedians <- rep(NA, nimputations)
preStDeviations <- rep(NA, nimputations)

recMeans <- rep(NA, nimputations)
recMedians <- rep(NA, nimputations)
recStDeviations <- rep(NA, nimputations)

f1sMeans <- rep(NA, nimputations)
f1sMedians <- rep(NA, nimputations)
f1sStDeviations <- rep(NA, nimputations)

for(i in 1:nimputations){
    accMeans[i] <- mean(accuracies[imputation_names[i]][ids_all_model_measures,])
    accMeans[i] <- round(accMeans[i], 3)
    accMedians[i] <- median(accuracies[imputation_names[i]][ids_all_model_measures,])
    accMedians[i] <- round(accMedians[i], 3)
    accStDeviations[i] <- sd(accuracies[imputation_names[i]][ids_all_model_measures,])
    accStDeviations[i] <- round(stDeviations[i], 3)
    
    preMeans[i] <- mean(precisions[imputation_names[i]][ids_all_model_measures,])
    preMeans[i] <- round(preMeans[i], 3)
    preMedians[i] <- median(precisions[imputation_names[i]][ids_all_model_measures,])
    preMedians[i] <- round(preMedians[i], 3)
    preStDeviations[i] <- sd(precisions[imputation_names[i]][ids_all_model_measures,])
    preStDeviations[i] <- round(preStDeviations[i], 3)
    
    recMeans[i] <- mean(recalls[imputation_names[i]][ids_all_model_measures,])
    recMeans[i] <- round(recMeans[i], 3)
    recMedians[i] <- median(recalls[imputation_names[i]][ids_all_model_measures,])
    recMedians[i] <- round(recMedians[i], 3)
    recStDeviations[i] <- sd(recalls[imputation_names[i]][ids_all_model_measures,])
    recStDeviations[i] <- round(recStDeviations[i], 3)
    
    f1sMeans[i] <- mean(f1s[imputation_names[i]][ids_all_model_measures,])
    f1sMeans[i] <- round(f1sMeans[i], 3)
    f1sMedians[i] <- median(f1s[imputation_names[i]][ids_all_model_measures,])
    f1sMedians[i] <- round(f1sMedians[i], 3)
    f1sStDeviations[i] <- sd(f1s[imputation_names[i]][ids_all_model_measures,])
    f1sStDeviations[i] <- round(f1sStDeviations[i], 3)
}

correct_measures_measures <- data.frame(imputation_names, accMeans, accMedians, accStDeviations,
                                preMeans, preMedians, preStDeviations,
                                recMeans, recMedians, recStDeviations,
                                f1sMeans, f1sMedians, f1sStDeviations)

knitr::kable(correct_measures_measures, content = "CORRECT model measures' concentrations' measures")
```

So now we can make some conclusions!

Having the table above, we can't say that *missForest* is definitely the best. Looking at the advanced ones, *missForest* and *VIM* are doing very well, almost always better than their competitors. *VIM* has slightly better accuracy and definitely better precision, while *missForest* dominates in the recall case and has slightly better f1. ***Mice* is almost always the worst**. Even *remove_rows* is doing better than it, and the *median* sometimes catches up with missForest and mice.

Standard deviations are pretty balanced and it's hard to tell anything clever about it - especially considering the small number of collections. Rather, nothing stands out.

Let's visualize dataset's model results measures.

```{r model_measures_plots, message = FALSE, cache = TRUE}
colors2 <- c("mean" = "blue", "median" = "orange")

acc_measures <- correct_measures_measures %>% select(imputation_names, accMeans, accMedians)
colnames(acc_measures) <- c("imputation_name", "mean", "median")
accMelt <- melt(acc_measures)
accuracyPlot <- ggplot(data = accMelt, aes(x = imputation_name, y = value, color = variable)) +
  geom_point(size = 3) + 
  ylim(0.75, 1) +
  theme_light() +
  ggtitle("Accuracies") +
  labs(color = "Legend") +
  scale_color_manual(name = "measure", values = colors2) +
  theme(legend.position="None",
        plot.title = element_text(hjust = 0.5),
        axis.title = element_blank())

pre_measures <- correct_measures_measures %>% select(imputation_names, preMeans, preMedians)
colnames(pre_measures) <- c("imputation_name", "mean", "median")
preMelt <- melt(pre_measures)
precisionPlot <- ggplot(data = preMelt, aes(x = imputation_name, y = value, color = variable)) +
  geom_point(size = 3) + 
  ylim(0.75, 1) +
  theme_light() +
  ggtitle("Precisions") +
  labs(color = "Legend") +
  scale_color_manual(name = "measure", values = colors2) +
  theme(legend.position="None",
        plot.title = element_text(hjust = 0.5),
        axis.title = element_blank())

rec_measures <- correct_measures_measures %>% select(imputation_names, preMeans, preMedians)
colnames(rec_measures) <- c("imputation_name", "mean", "median")
recMelt <- melt(rec_measures)
recallPlot <- ggplot(data = recMelt, aes(x = imputation_name, y = value, color = variable)) +
  geom_point(size = 3) + 
  ylim(0.75, 1) +
  theme_light() +
  ggtitle("Recalls") +
  labs(x = "Imputation name",
       y = "Recall",
       color = "Legend") +
  scale_color_manual(name = "measure", values = colors2) +
  theme(legend.position="None",
        plot.title = element_text(hjust = 0.5),
        axis.title = element_blank())

f1_measures <- correct_measures_measures %>% select(imputation_names, f1sMeans, f1sMedians)
colnames(f1_measures) <- c("imputation_name", "mean", "median")
f1mMelt <- melt(f1_measures)
f1sPlot <- ggplot(data = f1mMelt, aes(x = imputation_name, y = value, color = variable)) +
  geom_point(size = 3) + 
  ylim(0.75, 1) +
  theme_light() +
  ggtitle("F1s") +
  labs(x = "Imputation name",
       y = "f1",
       color = "Legend") +
  scale_color_manual(name = "measure", values = colors2) +
  theme(legend.position="None",
        plot.title = element_text(hjust = 0.5),
        axis.title = element_blank())

grid.arrange(accuracyPlot, precisionPlot,
             recallPlot, f1sPlot, nrow = 2,
             top = "Means (blue) and medians (orange) of datasets' imputations' modeling results")
```

I think we will all agree now - in terms of results, at least looking at these six datasets for which each imputation was successful, we can create such a podium:

1. *VIM* / *missForest* (judging by our need - for *accuracy* and *precision* - *VIM*, for *recall* - *missForest*)
3. *median*
4. *remove_rows*
5. *mice*

Of course, the results are not very representative, given the small number of datasets for which they were compared - but still something.

## Imputation time and its results

We collected and summarized both results and imputation times in our collections. What are the average imputation times and the measures' medians of our imputations?

```{r imputation_times_measures_medians, cache = TRUE}
medians_measures_times <- correct_measures_measures %>% select(imputation_names, accMedians, preMedians, recMedians, f1sMedians)
medians_measures_times <- cbind(medians_measures_times, imputation_times_measures_fullimp$medians)
colnames(medians_measures_times) <- c("imputation name", "accuracy", "precision", "recall", "f1", "imputation mean time")
knitr::kable(medians_measures_times, content = "Mean imputation time and mean model result's measures of imp methods")
```

Looking at the results, *VIM* and *missForest* give quite similar numbers in terms of satisfaction, but VIM is much faster - for data sets for which each imputation has passed, it managed in six times shorter period of time than *missForest* (and also of course much worse *mice*). The question is how much we are interested in time - it turns out that for very large data sets our model will not suffer much if we take the median instead of these two. At this point, you can probably make a correction of the ranking:

1. *VIM*
2. *median* / *missForest* (depending on how important time is for us)
...
4. *remove_rows*
...
5. *mice*

## Conclusion

The experiment can be considered successful when it comes to the datasets and tools made available to us - calculated measures, visualizations made, as well as conclusions led us, among others, to the intuition that probably *VIM* is really a very good package, supplementing the missing values with medians from columns is not at all is such a stupid idea as it may seem, and *mice* can be thrown in the bin. Unfortunately, it is impossible to draw a wise conclusion based on such a small number of tested data - admittedly as many as 14 sets, problems with imputations on some of them or too small dataframes not giving a satisfactory answer and assumptions that respectful statistics would praise. Nevertheless, above code is certainly very valuable and for more data sets it could confirm one or another belief - therefore we can probably say that we have achieved scientific success!

### References

*^1^mice: Multivariate Imputation by Chained Equations, package created by Stef van Buuren, Karin Groothuis-Oudshoorn, Gerko Vink, Rianne Schouten, Alexander Robitzsch, Lisa Doove, Shahab Jolani, Margarita Moreno-Betancur, Ian White, Philipp Gaffert, Florian Meinfelder, Bernie Gray*

*^2^VIM: Visualization and Imputation of Missing Values, package created by Matthias Templ, Alexander Kowarik, Andreas Alfons, Bernd Prantner*

*^3^missForest: Nonparametric Missing Value Imputation using Random Forest Daniel J. Stekhoven*

*^4^rpart: Recursive Partitioning and Regression Trees, package by Terry Therneau, Beth Atkinson, Brian Ripley*
