---
title: "Imputation methods' comparison"
author: "Miko≈Çaj Jakubowski, Marceli Korbin, Patryk Wrona"
date: "27th of April, 2020"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    code_folding: hide
    number_sections: true
---

```{r setup, include=FALSE}
library(VIM)
library(mice)
library(missForest)
library(dplyr)
library(microbenchmark)
library(OpenML)
knitr::opts_chunk$set(cache = TRUE)
```

## Objective

In the report, we will evaluate machine learning algorithms' performance based on the completion of datasets and several data imputation techniques.

## Methods

We are going to implement (using R language) and evaluate five imputation methods:  

* **deleting features** that contain any missing data;  
* completing columns with **mean** (numeric features), **median** (integer features) or **dominant** (factor features);  
* **IRMI** method from the VIM package;  
* **mice** imputation;  
* **missForest** imputation.

The first two are basic, manual methods, while the next three come from R packages designed for and focusing on working with data imputation. With such set, we give ourselves the opportunity to check the performance basing on the complexity of methods.

```{r imputing}
library(VIM)
library(mice)
library(missForest)
library(dplyr)

imputation <- function(df, num){
  # 1: removing columns with missing data
  # 2: imputing with: mean (continous variables),
  # median (ordinal variables)
  # modal value (nominal variables)
  # 3: IRMI from VIM
  # 4: mice
  # 5: missForest
  
  if (num==1)
    return(df[, colSums(is.na(df))==0])
  
  else if (num==2){
    classes <- sapply(df[, -ncol(df)], class)
    moda <- function(x){
      u <- unique(x)
      u <- u[!is.na(u)]
      u[which.max(tabulate(match(x, u)))]
    }
    nas <- colSums(is.na(df[, -ncol(df)]))
    for (i in 1:(ncol(df)-1)) if (nas[[i]]>0){
      if(classes[[i]]=="numeric")
        df[is.na(df[,i]), i] <- mean(df[[i]], na.rm=T)
      else if(classes[[i]]=="integer")
        df[is.na(df[,i]), i] <- median(df[[i]], na.rm=T)
      else
        df[is.na(df[,i]), i] <- moda(df[[i]])
    }
    return(df)
  }
  
  else if (num==3){
    fit <- VIM::irmi(df[, -ncol(df)], trace=F, imp_var=F, maxit=10) %>%
      cbind.data.frame(df[, ncol(df)])
    colnames(fit)[ncol(fit)] <- colnames(df)[ncol(df)]
    return(fit)
  }
  
  else if (num==4){
    fit <- mice(df[, -ncol(df)], m=2, maxit=3, printFlag=F)
    comp <- complete(fit) %>%
      cbind.data.frame(df[, ncol(df)])
    colnames(comp)[ncol(comp)] <- colnames(df)[ncol(df)]
    return(comp)
  }
  
  else if (num==5){
    fit <- missForest(df[, -ncol(df)], maxiter=3, ntree=15)
    comp <- fit$ximp %>%
      cbind.data.frame(df[, ncol(df)])
    colnames(comp)[ncol(comp)] <- colnames(df)[ncol(df)]
    return(comp)
  }
}
```

The methods will be compared through measuring the evaluation time. We make the measurement by using the microbenchmark function. Number of tests shall vary depending on the dataset, since its size happens to greatly affect the time of a single test irrespectively of the method.

```{r measure}
# source('./imputing_function.R')

measureImputationTime <- function(df, number_of_tests){
  measured <- microbenchmark::microbenchmark(
    removing = imputation(df, 1),
    mean_med_modal = imputation(df, 2),
    IRMI_VIM = imputation(df, 3),
    mice = imputation(df, 4),
    missForest = imputation(df, 5),
    times = number_of_tests
  )
  mean_times <- measured %>%
                group_by(expr) %>%
                summarize(mean_time = mean(time))
  return(mean_times)
}
```

As for the next step, we will train a machine learning algorithm on each of the imputation outputs for every dataset; this gives 40 trainings overall. We are testing a total of three algorithms: the gradient boosting algorithm, the random forest classifier and the logistic regression responses. The choice between them varies depending on the dataset.

### Loading data

Our 5 techniques are carried out on 8 datasets. All originate from the OpenML package and contain several missing values. Most of them, if not all, have already been analysed in previous works on the course. Their indexes are:

4, 27, 29, 38, 55, 56, 188, 944

Now we will load all above datasets as a list of data frames.

```{r, echo=FALSE, results='hide', message = FALSE, warning = FALSE}

source("./loading_data.R")
listOfDatasets <- loadAllSets()
```


### Imputation time comparison



```{r, message = FALSE, warning = FALSE}
# DATASET 4

measureImputationTime(listOfDatasets[[1]][[1]], number_of_tests = 5)
```

```{r, message = FALSE, warning = FALSE}
# DATASET 27

measureImputationTime(listOfDatasets[[2]][[1]], number_of_tests = 3)
```

```{r, message = FALSE, warning = FALSE}
# DATASET 29

measureImputationTime(listOfDatasets[[3]][[1]], number_of_tests = 3)
```

```{r, message = FALSE, warning = FALSE}
# DATASET 38

measureImputationTime(listOfDatasets[[4]][[1]], number_of_tests = 1)
```

```{r, message = FALSE, warning = FALSE}

# DATASET 55

measureImputationTime(listOfDatasets[[5]][[1]], number_of_tests = 5)
```

```{r, message = FALSE, warning = FALSE}

# DATASET 56

measureImputationTime(listOfDatasets[[6]][[1]], number_of_tests = 1)
```

```{r, message = FALSE, warning = FALSE}

# DATASET 188

measureImputationTime(listOfDatasets[[7]][[1]], number_of_tests = 5)
```

```{r, message = FALSE, warning = FALSE}

# DATASET 944

measureImputationTime(listOfDatasets[[8]][[1]], number_of_tests = 5)
```


## Machine learning models

Now, it is high time to evaluate the change of model performance across all data sets in function of used imputation algorithm:

* 1 = deleting features that contain any missing data;  
* 2 = completing columns with mean (numeric features), median (integer features) or dominant (factor features);  
* 3 = IRMI method from the VIM package;  
* 4 = mice imputation;  
* 5 = missForest imputation.

### Loading algorithms:

In order to assess imputation algorithms, we will compare the metrics of the below 3 machine learning models in function of used imputation:

- Random Forest
- Linear Regression
- Gradient Boosting

We have chosen 1 algorithm for each dataset basing on whether this dataset is large (Gradient Boosting) or small (Random Forest or Linear Regression).

We have already created scripts that take our data frames, then teach a model on it to finally assess the model's performance. They are named: "[model's name]_train_and_result.R".


```{r, echo=FALSE, results='hide', message = FALSE, warning = FALSE}

source('./gradient_boost_train_and_result.R')
source('./logistic_regression_train_and_result.R')
source('./random_forest_train_and_result.R')
```


### Dataset 1 - Random Forest

```{r, message = FALSE, warning = FALSE}
results <- list(NA)

# DATASET 4:
df <- listOfDatasets[[1]][[1]]
target <- listOfDatasets[[1]][[2]]
results[[1]] <- data.frame(acc = NA, f1 = NA, prec = NA, rec = NA)
results[[1]] <- rbind(results[[1]], data.frame(acc = NA, f1 = NA, prec = NA, rec = NA))
for(i in 3:5){
  imp <- imputation(df, i)
  aux <- getRFResults(imp, target)
  results[[1]] <- rbind(results[[1]], aux)
}

results[[1]]
```

### Dataset 2 - Linear regression


```{r, message = FALSE, warning = FALSE}
# DATASET 27:
df <- listOfDatasets[[2]][[1]]
target <- listOfDatasets[[2]][[2]]
imp <- imputation(df, 1)
results[[2]] <- getLRResults(imp, target)
for(i in 2:5){
  imp <- imputation(df, i)
  aux <- getLRResults(imp, target)
  results[[2]] <- rbind(results[[2]], aux)
}

results[[2]]
```

### Dataset 3 - Random Forest

```{r, message = FALSE, warning = FALSE}
# DATASET 29:
df <- listOfDatasets[[3]][[1]]
target <- listOfDatasets[[3]][[2]]
imp <- imputation(df, 1)
results[[3]] <- getRFResults(imp, target)
imp <- imputation(df, 2)
results[[3]] <- rbind(results[[3]], getRFResults(imp, target))
imp <- imputation(df, 3)
results[[3]] <- rbind(results[[3]], getRFResults(imp, target))
results[[3]] <- rbind(results[[3]], NA)
imp <- imputation(df, 5)
results[[3]] <- rbind(results[[3]], getRFResults(imp, target))

results[[3]]
```

### Dataset 4 - Gradient Boosting

```{r, message = FALSE, warning = FALSE}
# DATASET 38:
df <- listOfDatasets[[4]][[1]]
target <- listOfDatasets[[4]][[2]]
imp <- imputation(df, 1)
results[[4]] <- getGDBRestults(imp, target)
for(i in 2:5){
  imp <- imputation(df, i)
  aux <- getGDBRestults(imp, target)
  results[[4]] <- rbind(results[[4]], aux)
}

results[[4]]
```

### Dataset 5 - Linear regression

```{r, message = FALSE, warning = FALSE}
# DATASET 55:
df <- listOfDatasets[[5]][[1]]
target <- listOfDatasets[[5]][[2]]
imp <- imputation(df, 1)
results[[5]] <- getLRResults(imp, target)
for(i in 2:5){
  imp <- imputation(df, i)
  aux <- getLRResults(imp, target)
  results[[5]] <- rbind(results[[5]], aux)
}

results[[5]]
```

### Dataset 6 - Linear regression

```{r, message = FALSE, warning = FALSE}
# DATASET 56:
df <- listOfDatasets[[6]][[1]]
target <- listOfDatasets[[6]][[2]]
imp <- imputation(df, 1)
results[[6]] <- data.frame(acc = NA, f1 = NA, prec = NA, rec = NA)
results[[6]] <- rbind(results[[6]], data.frame(acc = NA, f1 = NA, prec = NA, rec = NA))
results[[6]] <- rbind(results[[6]], data.frame(acc = NA, f1 = NA, prec = NA, rec = NA))

for(i in 4:5){
  imp <- imputation(df, i)
  aux <- getLRResults(imp, target)
  results[[6]] <- rbind(results[[6]], aux)
}

results[[6]]
```

### Dataset 7 - Gradient Boosting

```{r, message = FALSE, warning = FALSE}
# DATASET 188:
df <- listOfDatasets[[7]][[1]]
target <- listOfDatasets[[7]][[2]]
imp <- imputation(df, 1)
results[[7]] <- getGDBRestults(imp, target)
for(i in 2:5){
  imp <- imputation(df, i)
  aux <- getGDBRestults(imp, target)
  results[[7]] <- rbind(results[[7]], aux)
}

results[[7]]
```

### Dataset 8 - Random Forest

```{r, message = FALSE, warning = FALSE}
# DATASET 944:
df <- listOfDatasets[[8]][[1]]
target <- listOfDatasets[[8]][[2]]
results[[8]] <- data.frame(acc = NA, f1 = NA, prec = NA, rec = NA)
results[[8]] <- rbind(results[[8]], data.frame(acc = NA, f1 = NA, prec = NA, rec = NA))
for(i in 3:5){
  imp <- imputation(df, i)
  aux <- getRFResults(imp, target)
  results[[8]] <- rbind(results[[8]], aux)
}

results[[8]]

```




## Results

Thanks to our results, we can deduce the best techniques of imputation that appeared in each dataset of our research:

Datasets and their respective best imputation methods (1st and 2nd place ranking):

1) IRMI(3), missForest(5)
2) IRMI(3), mean/median/dominant(2)
3) missForest(5), IRMI(3)
4) missForest(5), mean/median/dominant(2)
5) missForest(5), mice(4)
6) missForest(5), mice(4)
7) IRMI(3), mice(4)
8) IRMI(3), missForest(5)

Looking simply on the best imputations is not enough because there would be 2 winners. Nevertheless, judging by two best imputation techniques, one can deduce that the most occurring was **missForest(5)** imputation. On the other hand, the worst imputation technique was **deleting columns with NA(1)** which could be seen after comparing performance metrics. Indeed, more complex techiques (*missForest* & *IRMI*) often bring better results, while a realtively simple technique (in this case - *deleting columns*) is the fastest and usually the worst imputation technique.

